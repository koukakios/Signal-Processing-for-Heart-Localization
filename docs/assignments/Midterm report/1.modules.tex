\chapter{Modules}
Place the results of the Modules each in a separate section. You don’t have to repeat everything
from a module, but give sufficient context to make your report independently readable. Embed the
assignments in a natural way, and give them intelligible names (not “Task 1”). You can combine
modules if that makes sense.

\section{Module 1}

\subsection{Specifications}
%Specifications: what is given, notation


In this module, the aim is to pre-process the data measured by the microphones. This data is a PCG (phonocardiogram), i.e. it represents the pressure of the sounds generated by the heart over time. The purpose of this module is to split the recordings into S1 and S2 peaks separately, which is useful for later modules. The S1 and S2 are peaks that can be observed when valves of the heart are opening and closing.





\subsection{Analysis}
%Analysis: which problems have to be overcome

To achieve the aim of this module, we need to detect where the wanted peaks are, and then isolate those peaks. It is important that peaks that aren't S1 and S2 peaks are not selected, as that would result in faulty data. Moreover, peaks that are S1 or S2 peaks should not be skipped, as this would result in excluding data that could be necessary and might have an influence on the detection of surrounding peaks. Another challenge is distinguishing S1 peaks from S2 peaks, as they have to be separated from each other. A possibility is to look at the timing of the peaks, using the knowledge that they originate from heart valves.

\subsection{Design}
%Design: what needs to be designed, approach

Fig. \ref{fig:model1overview} shows the steps that will be taken in the design of the peak detection. After filtering out high frequency components and extremely low frequency components, the energy of the signal will be determined. Using the envelope of this energy, the peaks will be detected using a threshold. These steps are given in [\cite{IP3Man}]. It should also be determined which peaks are S1 peaks and which S2, and a dynamic threshold can also increase accuracy.  

\begin{figure} [H]
    \centering
    \includegraphics[width=0.9\textwidth]{docs/assignments/Midterm report/figures/module_1/module1overview.png}
    \caption{Overview of module 1, as given in \cite{IP3Man}}  
    \label{fig:model1overview}
\end{figure}

The filtration should not affect the accuracy of the result, as only irrelevant frequencies will be filtered out. To ensure quick calculation, the signal will be downsampled, which results in resolution loss.
There are multiple ways to calculate the energy of the signal, which will influence the result in a way. However, in case the rest of the system is designed knowing how the energy is being calculated, it too shouldn't affect the accuracy. Constructing the envelope will be done using a filter, filtering out high frequencies and therefore keep a smooth envelope. A good cutoff frequency should be chosen so as to ensure that no data is being filtered out, and as little as possible noise is being kept. The threshold does have a big influence on the accuracy, as it will determine whether peaks will be considered or not.

%It is highly appreciated if you include an analysis that explains how accurate your system could be (or will be), in view of hardware limitations.

\subsection{Resulting Design}


The main location where the system is implemented is in the "Processor" class, under the "run" function (\ref{ap:libprocessor}). This function calls other functions of the "Processor" class, enabling those functions to share variables very easily. These functions will now be explained in more detail. Some of these functions also use functions defined in "functions.py" (\ref{ap:functions}) and "dataprocessing.py" (\ref{ap:dataprocessing}).
\subsubsection{Data acquisition and filtering}
In the "load" function, the data is first read from the audio file using the "wavfile" function from the scipy library and put in the variable "x". Next, the function "preprocess" is called, which constructs a bandpass filter with cut-off frequencies at 10Hz and 800Hz, a filter order of 2 and a size of 5000 samples. These values are listed in "config.ini" (\ref{ap:config}). A butterworth implementation from the signal library was used to create the filter. The filter is then applied to the data using the "convolve" function of the NumPy library, resulting in the variable "y". After that, the signal is downsampled to 4000 Hz (also listed in "config.ini" (\ref{ap:config})), putting the resulting signal in "y\_downsampled". Before computing the energy of the signal, the signal is normalized to ensure correct computation of the energy, resulting in the variable "y\_normalized".

\subsubsection{Shannon energy}
Next, the energy of the signal is computed by the function "preprocess" using the Shannon energy formula:
\begin{equation}
    \label{eq:shannonenergy}
    E(t) = - x_n^2(t) log(x_n^2(t))
\end{equation}
The variable "y\_normalized" was used for $x_n(t)$. The result is put in the variable "y\_energy". 

\subsubsection{Shannon energy envelope}
Following that, the envelope of the energy is taken by applying a lowpass filter with a cut-off frequency of 10Hz, an order of 2 and a size of 1000 samples. These values are also listed in "config.ini" (\ref{ap:config}). The result is put in the variable "see". This envelope is then normalized by ensuring the standard deviation is 1, and the result is put in the variable "see\_normalized". This will be useful in future steps. 

\subsubsection{Peak detection}
The function "preprocess" ends by retreiving the peaks in the envelope using the "find\_peaks" function from the signal library. The threshold is set to 0.4, i.e. 40\% of the standard deviation, and the minimum distance between peaks is set to 0.2 seconds (both listed in "config.ini" (\ref{ap:config})). The results of this function are stored in the variables "peaks", representing at which sample, which from now on will be called the "x" value of a peak, the peaks occur, and "peak\_properties", representing properties of the peaks, of which only the heights will be used. 

% classify peaks
\subsubsection{"Classify\_peaks" function}
Next, in the function "classify", the function "classify\_peaks" calculates the differences between these "x" values, and the differences between those differences, and stores these values in the variables "diff" and "diff2", respectively.  These variables are combined into one data structure called "peaks". In this data structure, the "diff" value of an element represents the difference of the current x value and the next x value, and the "diff2" value represents the difference between the current "diff" value and the next "diff" value. 
\subsubsection{"Analyze\_diff2" function}
This data structure is then passed to the function "analyze\_diff2", which first classifies each peak as a maximum or minimum using the "diff" and "diff2" values. Then, a variable "y\_line" is calculated by iteratively removing maxima and minima until the highest "diff" value of a minimum is lower than the lowest "diff" value of a maximum. The average of these two "diff" values is then chosen. \newline
In ideal conditions, only one iteration will be carried out as all "diff" values of the minima will be lower than all "diff" values of the maxima. However, when faulty peaks are detected or relevant peaks are not detected, it can occur that the "diff" value of a minima will be higher than the "diff" value of a maxima. In that case, more iterations are carried out to still be able to generate a value for "y\_line", keeping in mind that this might result in less accurate detection later on, although this loss is minimized by removing the highest "diff" values of minima and lowest "diff" values of maxima first, as these are most likely to be faults.\newline
Using the value of "y\_line", all the maxima and minima are classified as S1 peak, S2 peak or uncertain. This is done by comparing the "diff" value of each peak to the "y\_line" variable. If the "diff" value of a peak is larger than the y\_line, then the peak is classified as an S2 peak, under the condition that the previous peak wasn't an S2 peak. If the "diff" value of a peak is lower than the y\_line, then the peak is classified as an S1 peak, under the condition that the previous peak wasn't an S1 peak. If neither condition satisfy, the peak is classified as uncertain. \newline
This was done, because the difference between an S1 peak and S2 peak is almost always smaller than from an S2 peak to the next S1 peak, due to physical reasons of how a human heart works. When all peaks are classified, peaks that are right after uncertain peaks will also be classified as uncertain. \newline
The "analyze\_diff2" function returns 3 arrays containing the x values of S1 peaks, S2 peaks and uncertain peaks, which are stored in the variables "s1\_peaks", "s2\_peaks" and "uncertain", respectively. The steps of the functions "classify\_peaks" and "analyze\_diff2" have been summarized in [\ref{fig:classificationchart}].



% troubleshooting
\subsubsection{Global threshold adjustment}
After calling the "classify\_peaks" function, a loop is entered, that increases the threshold for the "find\_peaks" function each iteration, and recomputes the number of uncertains using the "classify\_peaks" function. This loop is terminated when the number of uncertains is low enough or a maximum number of iterations is reached. A threshold of 20 uncertains each minute was determined, listed in "config.ini" (\ref{ap:config}), which is about 17\% of the expected 120 peaks each minute (60 BPM and 2 peaks for each heartbeat), and thus requiring a success rate of 83\%. The maximum number of iterations was determined at 100, which is also listed in "config.ini" (\ref{ap:config}). 
\subsubsection{"Solve\_uncertains" function}
Then, the function "solve\_uncertains" is called. This function aims to classify the remaining uncertain peaks as either S1 or S2 peaks. It does this by grouping adjacent uncertain peaks together. \newline
For each group, it then first checks if any of the peaks in the group have a "diff" value of less than 700 samples or if the "diff" value of the first peak in the group is less than the y\_line value. If this check is passed, it is known that too many peaks are detected, so then another loop is entered, which, in each iteration, removes the peak with the smallest height in the group, and recomputes the classifications for the peaks using the "classify\_peaks" function. It puts these classifications in the variables "s1\_peaks\_new", "s2\_peaks\_new", and "uncertain\_new". The loop terminates if all the uncertains in the group are resolved, or if all the peaks in the group are removed by the loop, meaning that no uncertain peaks were possible to be classified as an S1 or S2 peak. \newline
If the check is not passed, it is known that not enough peaks have been detected, which must have resulted in a peak with a "diff" value that is too high. This is because if a peak is removed (or not detected), the "diff" value of the peak before that increases by the "diff" value of the removed peak, which is apparent by the definition of the "diff" value.\newline 
A segment of the Shannon energy envelope is taken, which includes the peaks of the group and a padding of 2 seconds of samples on each side, defined by "SolveUncertainLength" in "config.ini" (\ref{ap:config}). Then, the "find\_peaks" function is used to detect all peaks in the segment, where the threshold requirement is omitted, and only the "min\_dist" argument is given, to not detect a very unrealistic number of peaks. The peaks that are detected in this way are compared to the peaks that were already in the group, and the difference is put in the variable "new\_peaks". \newline
Then a loop is entered, in which each iteration the peak in "new\_peaks" which has the biggest height in the Shannon energy envelope is added to the peaks in the group, and the peaks are classified again using the "classify\_peaks" function. The loop terminates when either all the peaks in "new\_peaks" have been added (which is a failure), or there are no uncertain peaks left (which is a success). \newline
At the end, the "solve\_uncertains" function returns the new arrays for "s1\_peaks", "s2\_peaks" and uncertains. The steps regarding the global threshold adjustment and the "solve-"uncertains" function have been summarized in [\ref{fig:solve_uncertains_chart}].


% segmenting
\subsubsection{Segmentation}
Lastly, using the indices of the peaks and a threshold, defined as "EnvelopeThreshold" in "config.ini" (\ref{ap:config}), domains are generated where the whole peaks occur. This is done by first generating segments which have a Shannon energy higher than the threshold, and then keeping the ones that include peaks, as it can easily be checked if the index of a peak is in a segment. \newline
As these segments contain indices for samples, they can be applied on either "x" or "y\_normalized", i.e. the original signal or the filtered and downsampled signal. Multiple files are generated using these segments: files for the S1 peaks and the S2 peaks, and by applying those segment on "x" or "y\_normalized", also called "raw" and "processed". There are also two ways the segment indices can be applied: either the samples that are not in the segments are set to zero, or are omitted altogether, resulting in a shorter signal. This means that there will be 8 files generated.\newline
The class "Executor" is able to loop through multiple files, and call the "Processor" class for each file. It also organizes the output files in folders for ease of use.


\subsection{Testing}
%Testing: how do we test, what are the results? (plots!)

The classification algorithm was tested by applying it on a data set of heart recordings. The results of the algorithm were checked by hand. It was checked if the algorithm missed peaks where there were peaks, or marked peaks where there were no peaks present. The latter did only occur twice. The results using the data acquired using a piezo electric element are presented in table [\ref{tab:test_piezo_segmentation}], the results using the data acquired using a stethoscope are presented in table [\ref{tab:test_stethoscope_segmentation}]. Using this data, the accuracy of the algorithm can be computed. As there are a lot of samples (almost all of them) which were not marked as peaks, type I errors are neglected, of which only two were made.

To answer the question: "If I take a random peak (S1/S2) in the sound signal, what is the chance this peak is correctly classified?", this test data can be summarized as seen in tables \ref{tab:piezo_results} and \ref{tab:stethoscope_results}. Table \ref{tab:piezo_results} summarizes the results for the piezo recordings, and table \ref{tab:stethoscope_results} those of the stethoscope recordings. "Uncertain/skipped" indicates the percentage of the peaks that were skipped, or marked as uncertain and therefore skipped too in further processing. "Incorrectly labeled peak" treats the cases where a S1 peak was labeled as an S2 peak, or vice versa. A "correctly labeled peak" is a peak that was correctly classified. 

As can be seen in the tables, no peaks were incorrectly classified. Another remarkable result is that the algorithm performs at least 6 times better on stethoscope recordings as compared to the piezo recordings. 

\begin{table}[ht!]
\scriptsize
\centering
\captionsetup{justification=centering}
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{S1} & \textbf{S2} \\
\hline
Uncertain/skipped & 2.67\% & 3.86\% \\
Incorrectly labeled peak & 0.00\% & 0.00\% \\
Correctly labeled peak & 97.33\% & 96.14\% \\
\hline
\end{tabular}
\caption{Classification accuracy of piezo recordings.}
\label{tab:piezo_results}
\end{table}

\begin{table}[ht!]
\scriptsize
\centering
\captionsetup{justification=centering}
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{S1} & \textbf{S2} \\
\hline
Not marked as correct peak & 0.44\% & 0.45\% \\
Incorrectly labeled peak & 0.00\% & 0.00\% \\
Marked as correct peak & 99.56\% & 99.55\% \\
\hline
\end{tabular}
\caption{Classification accuracy of stethoscope recordings.}
\label{tab:stethoscope_results}
\end{table}

\subsection{Conclusion}
%Conclusion: summarize results, claims on expected accuracy


\section{Module 2}
\subsection{Specifications}
%Specifications: what is given, notation
The aim of this module is to generate a model of the human heart, and be able to generate test data using the model. 

\subsection{Analysis}
%Analysis: which problems have to be overcome
There are many parts in a human heart that generate sound. It should be examined how these parts generate sound, and thus how is a sound signal generated by that part. Also, it should be examined which parts are relevant to model.

\subsection{Design}
%Design: what needs to be designed, approach
%It is highly appreciated if you include an analysis that explains how accurate your system could be (or will be), in view of hardware limitations.
For simplicity, it is convenient to model the heart as 4 valves. Then, a generic valve model can be developed, which can be used by all 4 valves, but using different parameters. It is possible use an impulse response as a model for a valve, and use a number of poles to generate it. After generating a response for each valve, the damping and delay to each microphone should be computed using the distance from each valve to the microphone. Concluding, for each microphone, all the responses must be added into one signal. These steps have been given by \cite{IP3Man}. It is also possible create an environment to adjust the parameters manually to create a realistic model.

\subsection{Resulting design}
%The resulting design: implementation, main variables used
\subsubsection{Single valve model}
For the single valve model, two transferfunctions based on two poles are created. One to create an onset, and one as main transferfunction. The impulse response of each transferfunction was created using two poles, $a-j\omega$ and $a+j\omega$, where a is the damping of the function and $\omega$ is the radial frequency of the function. The transferfunction is then truncated based on the "duration" parameter, and multiplied by the parameter "ampl", to be able to implement a relative damping of the valve (with respect to the other 3 valves). After both transferfunctions are generated, they are concatenated, and preceded by a number of zero's, based on de "delay" parameter. This delay parameter only defines the delay in time relative to the start of a heart beat. This result in a transferfunction for one valve.


\subsubsection{Optimizing model parameters}
In order to optimize the parameters of the model, an interface was made. This interface uses the terminal in combination with a matplotlib plot. Writing commands in the terminal adjusts the parameters, which are immediately visible in the plot. 
A good measurement was plotted together with the model. This was used to adjust the parameters such that the model approximately follows that signal. The resulting model and a measurement have been plotted in \ref{fig:modeled_signal}.

\subsubsection{Randomizing}
In order to not generate purely identical heart beats, an option was made to randomize the parameters of the model slightly and add noise. However, this severely slows down the generation of the model, and therefore it was not used in the plot.


\subsubsection{Spatial delay}
To incorporate the delay and damping caused by the spatial difference between the location of a valve and the location of a microphone, the distance between them has to be calculated. This is done by using the Pythagoras Theorem for all 24 valve-microphone combinations. The distance is used to calculate a damping, using the relation $a \propto d$, where $a$ is the damping (unitless), and $d$ is the distance. 

\subsubsection{Overall integration}
As it is required to output 6 signals, one signal for each microphone, the function "generate" in "Model\_3D.py" (\ref{ap:Model_3D}) loops through the microphones. For each microphone, it loops through the valves, calculates the single valve model for that valve, pads and repeat that model to generate a certain amount of beats and BPM as defined in "config.ini" (\ref{ap:config}), adjusts the model for the spatial delay, and adds the responses of all 4 valves together to get one signal. 




\subsection{Testing}
%Testing: how do we test, what are the results? (plots!)

The single valve model was tested in to ways. Firstly, it was compared to an actual measurement. As the model parameters were generated based on a measurement, the model resembles the used measurement quite closely. The model was also compared to other signals, which, other than having slightly different heart beat rates, also coincide quite closely. In Fig. \ref{fig:modeled_signal} this plot is shown for one heart beat. 
\newline
The single valve model was also saved as an audio file and listened to. Based on subjective opinions, the model was found to sound quite an actual heart, as the characteristic "lub-dub" sound was heard.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.9\textwidth]{docs/assignments/Midterm report/figures/module_2/Modeled_signal.png}
    \caption{Measurement of one heart beat and the signal of the resulting single valve model.}  
    \label{fig:modeled_signal}
\end{figure}

The damping and delay of the multichannel recording were tested by inspection. As can be seen in Fig. [\ref{fig:multichannel_damping}], the model generates signals with different amplitudes for the microphones. In Fig. [\ref{fig:multichannel_delay}], the delay created by the model is visible. The small delay is expected, as the distances between the valves and the microphones is very small.



\subsection{Conclusion}
%Conclusion: summarize results, claims on expected accuracy
Concluding, a model of the sound the heart creates has been made. This model has been applied to 6 microphones, where the spatial differences between microphones and valves has also been accounted for. The resulting model seems to be quite accurate, as the single valve model looks and sounds like an actual heart beat, and the spatial results seem realistic.

\section{Module 3}
\subsection{Beamformer} 

\subsubsection{Specifications}
The goal of this module is to create a system that is able to determine in which direction the source of a signal is located. There are multiple approaches to this problem, the ones discussed here are the beamformer and MVDR. The implemented system should use files of multiple microphones as input and the output will be a plot that shows the position of the signal's source.

\subsubsection{Analysis}
\paragraph{Signal model}
Every microphone records an identical but shifted audio response and the angle that the source is located determines how much the signal is shifted between each microphone. This means that the output array of the microphones can be modeled as shifted versions of the original signal. This means that the responses can be re-shifted the back to the original position by a specific phase. If this new phase shift matches the original phase shift, the power response should be highest.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.9\textwidth]{docs/assignments/Midterm report/figures/module_3/Beamformer/beamformer_model.png}
    \caption{Overview of  beamformer system, as given in \cite{IP3Man}}  
    \label{fig:beamformer_principle}
\end{figure}

The image implies that the signal can be modeled as \( \mathbf{x}(t) = \mathbf{a}(\theta)\, s(t) + \mathbf{n}(t) \). \( \mathbf{x}(t) \) is the microphone vector, \( s(t) \) is the source signal, \( n(t) \) is additive noise, and \( \mathbf{a}(\theta) \) is the steering vector that captures the phase shifts between microphones.

Specifically, \( \mathbf{a}(\theta) \) comprises of complex exponentials, which express the delay/phase shift of arrival of the signal at the different microphones. This phase shift/delay depends on the angle of arrival of the signal \( \mathbf(\theta) \), the spacing between the microphones, d, and the frequency of the signal f. 


Thus, as explained by \cite{IP3Man}, the steering vector \( \mathbf{a}(\theta) \) can be modeled by the following equation.
\[
 \mathbf{a}(\theta) = \begin{bmatrix}
1 \\
e^{-j(d/v)\sin(\theta)\Omega_0} \\
e^{-j2d/v\sin(\theta)\Omega_0} \\
\vdots \\
e^{-j(M-1)(d/v)\sin(\theta)\Omega_0}
\end{bmatrix}.
\]
\label{steering_vector_calculation}

Here, $\theta$ expresses the direction of arrival, $d$ the microphone spacing, $v$ the speed of the wave (sound in this case), and $\Omega_0$ the radial frequency.

\paragraph{Beamforming principle}
The delay and sum beamformer was used. What the beamformer does, according to \cite{IP3Man}, it synchronizes/aligns signals that come from a given angle $\theta$ and it doesn't alter signals from other directions (e.g. noise in this case). Then it sums them up, so the signals from a wanted angle add up in phase (constructively), while the unwanted signals add up destructively thus becoming less important.

According to \cite{IP3Man}, for the given signal of unity power $\mathbf{x}$ and a given angle of arrival $\theta_0$, the output $y$ is
\[
y(t) = \mathbf{a}(\theta)^H \mathbf{x}(t) = \mathbf{a}(\theta)^H \mathbf{a}(\theta_0) s_0(t),
\]
and the output power is
\[
P_y(\theta) = \left| \mathbf{a}^H(\theta)\mathbf{a}(\theta_0) \right|^2 \cdot 1.
\]
By trying all the angles $\theta$, the power will be maximum when the angle tried is the same as the angle of arrival, because this is when the correct alignment happens and the signals received by the microphones are summed up in phase. 



\paragraph{Assumptions, constraints and problems to overcome}\label{beamformer_assumption_constraints}
In order to use the beamformer model the following assumptions, constraints and problems were taken into account:
\begin{itemize}
    \item It was assumed that the source is far away from the microphones. This is the far field assumption, thus the sound waves coming from the source can be taken as parallel.
    
    \item Narrow-band condition. According to \cite{IP3Man}, for the signal bandwidth B it needs to hold : \[
    B \ll \frac{v}{D}, \quad \text{where } D = (M-1)d.
    \] D here is the length of the microphone array. Because this condition does not hold for the signals of this project, a bandpass filter centered at a chosen frequency needs to be applied. This is analyzed more in the design section.
    
    \item Spatial aliasing. According to \cite{IP3Man}, each microphone samples the input signal in time. However, the microphone array samples the input
    signal in space, and just as with sampling in time, the issue of aliasing comes up. To avoid this, spatial sampling needs to happen more densely than half the wavelength. So \[
    \Delta = \frac{d}{\lambda} < \frac{1}{2}
    \] 
    where $\Delta$ is the length measured in wavelengths, needs to be smaller than 1/2.\label{aliasing}

    \item Resolution - Aliasing tradeoff. As proven by \cite{IP3Man}, the highest the central frequency chosen, the better the resolution. This means that the lobes of the peaks of graphs are more sharp. On the other hand, if the central frequency is increased a lot then the wavelength $\lambda$ becomes smaller and thus the $\Delta$ bigger which leads to aliasing. So at low frequencies, 2 lobes might be merged with each other and be indistinguishable, but in high frequencies they might be disrupted by destructive aliasing.   

\end{itemize}


\subsubsection{Design}
Design: 
The resulting design: implementation, main variables used

As proven by \cite{IP3Man}, the Power of a signal \textbf{x} after beamforming is \[
P_y(\theta) = \mathbf{a}^H(\theta) \mathbf{R}_x \mathbf{a}(\theta)
.\]
Here $\alpha$ is the steering vector and $R\_x$ is the auto covariance of the signal.
Thus the $\theta$ where the power is maximized is our direction of arrival.

\subsubsection{Analysis of the functions used in beamforming}
\begin{itemize}
    \item The function "a\_lin" takes the arguments "theta", an angle, "M", the number of microphones, "d", the spacing between the microphones, "v", the speed of sound, and "f0", a frequency. This function computes and returns the steering vector $\mathbf{a}(\theta)$, using the steps described in \ref{steering_vector_calculation}.

    \item The function "matchedbeamforming" uses the same arguments "M", "d", "v" and "f0". It also takes an argument "Rx", which is the auto covariance. For a given range of angles (given by the argument "th\_range"), the function computes the output power and returns the power spectrum.
\end{itemize}

The pipeline for applying beamforming can be seen in \ref{fig:beamformer_pipeline}. It is thus apparent that the central frequency selection and the computation of the STFT (Short Time Fourier Transform) are crucial steps. Both of these are explained in \cite{IP3Man}. The core idea is that the STFT transforms the signal into the frequency domain. It essentially decomposes the signal into separate frequency bins, where each bin represents a narrow band version of the original signal evolving over time. 

\paragraph{Bandwidth selection}
As explained in \ref{beamformer_assumption_constraints}, B needs to be smaller than $\frac{v}{d*(M - 1)}$. For this project, $d = 0.1 m$, $M = 6$ and $v = 343 m/s$. Thus B needs to be smaller than 686 \textbf{Hz}.
The formula given from \cite{IP3Man} for the effective bandwidth of each bin is : \begin{equation}
    B = \frac{f_s}{\texttt{samples per bin}}
\end{equation}
More samples per bin mean lower B, safer narrow band assumption but lower resolution while less samples per bin mean higher B, worse narrow bandwidth assumption but increases resolution.
By making the samples per bin 256 and the sampling frequency ($f_s$) 48 kHz, B results in 187.5 Hz, which is way smaller than 686 Hz.

\label{central_frequency_selection}
\paragraph{Central frequency selection}
As explained in \ref{beamformer_assumption_constraints}, there is a tradeoff between spatial aliasing and resolution. Thus the best possible solution is to sample as close to the limit, thus close to half wavelength. This results in a central frequency close to 1715 Hz as seen in [\ref{module_3_calculation_central_frequency}]. Given the calculated B  (187.5 Hz) the best possible feasible bin frequency is 1687.5 Hz.



\subsubsection{Testing}
For testing the beamformer, measurements with one signal source at  $0^\circ$, $30^\circ$ and $60^\circ$ were made, but also for 2 sources located at $+7^\circ$ and $-7^\circ$. 


\paragraph{One source} For one source at $0^\circ$, the beamformer performs satisfactory. There is an error of $0.45^\circ$.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/assignments/Midterm report/figures/module_3/Beamformer/Beamformer 1 source/Beamformer_0.png}
    \caption{Beamformer at 0 $^\circ$.}  
    \label{fig:beamformer_0_degrees}
\end{figure}

The $30^\circ$ plot shown in [\ref{fig:beamformer_30_degrees}] shows that the accuracy is as desired.

The $60^\circ$ plot as shown in [\ref{fig:beamformer_60_degrees}] shows there is an offset of around $2^\circ$. This might be because of the setup. More specifically, the angle might not exactly have been $60^\circ$. Also the angle to the source in the setup is measured from the middle of the array, while in \ref{fig:beamformer_principle}, the angle is taken from the first microphone of the array.


\paragraph{2 sources}

\label{beamformer_resolution_2_sources_problem}
As explained in \label{beamformer_assumption_constraints}, there is a tradeoff between spatial aliasing and resolution. If there are 2 sources in a signal, the final power spectrum plotted for the beamformer, is practically the sum of the power spectrum for each source as if they were received in different signals. So the power spectrum of 2 sources is the sum of 2 power spectra of one source like \ref{fig:beamformer_0_degrees}. If the central frequency is lower, each of the lobes is thicker/more spread out. Thus the peak of their sum is somewhere in between, and the 2 sources are indistinguishable. This can be seen below.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/assignments/Midterm report/figures/module_3/Beamformer/Beamformer 2 source/Beamformer_1687Hz.png}
    \caption{Beamformer at +7$^\circ$, -7$^\circ$, low resolution.}  
    \label{fig:beamformer_2_source_1687Hz}
\end{figure}

If the central frequency is higher then the lobes created by the 2 sources are sharper and are not merged in one thick lobe. Thus there are 2 peaks but also spatial aliasing.In this case it is known that the 2 peaks seen are not result of aliasing. In lower frequencies \ref{fig:beamformer_2_source_1687Hz}, the peak of the power is around the center of the graph. Thus most of the signal lies there. As a result, it it known that the peaks caused by the 2 sources will occur in that region. The plot was expected, as 2 peaks are visible but they are still sort of merged, justifying the operation of the beamformer. 

\begin{figure} [H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/assignments/Midterm report/figures/module_3/Beamformer/Beamformer 2 source/Beamformer_3000Hz.png}
    \caption{Beamformer at +7$^\circ$, -7$^\circ$, higher resolution.}  
    \label{fig:beamformer_2_source_3000Hz}
\end{figure}





\subsection{MVDR}

\subsubsection{Analysis}
The MVDR beamformer works differently from the one described previously. The objective of implementing an MVDR is to choose a direction from where the source is coming and to minimize the gain of the rest of the angles. This translates to two requirements that must be implemented:
\begin{enumerate}
\item The strength of a signal coming from a chosen angle $\theta_0$ should not be reduced.
\item The response of the signal coming from any other angle should be minimized.
\end{enumerate}
In practice, $\theta_0$ can be multiple $\theta$'s

\subsubsection{Design}


The two requirements were implemented as the following constraints:

\begin{equation}
\textbf{w}^\textbf{H}\textbf{a}(\theta_0)=1
\end{equation}

This constraint has a specified $\theta_0$ and the gain of the signal that comes from this angle should not be reduced, whereas the gain from all other angles should be reduced as much as possible. This results in the second constraint:

\begin{equation}
\textbf{w}_{opt}=arg\: min\:\textbf{w}^\textit{H}\textbf{R}_x\textbf{w}
\end{equation}


The solution of these two constrains results in the following:

\begin{equation}
\textbf{w}_{opt}=\frac{\textbf{R}_x^{-1}\textbf{a}(\theta)}{\textbf{a}^\textit{H}(\theta)\textbf{R}_x\textbf{a}(\theta)}
\end{equation}
The power response of the system will be the output:

\begin{equation}
    P_y(\theta)=\textbf{w}^\textit{H}\textbf{R}_x\textbf{w}=\frac{1}{\textbf{a}^\textit{H}(\theta)\textbf{R}_x^{-1}\textbf{a}(\theta)}
\end{equation}
The intuition behind the power equation needs to be explained. In practice, 
$R_x$ expresses the power of the signal along different directions $\theta$. 
This means that $R_x$ is bigger where the power of the signal is larger. 
Thus $R_x^{-1}$ is smaller along the direction where the power is larger, 
and bigger where the power is lower, as it is the inverse of $R_x$. 
Thus when $\mathbf{a}(\theta)$ points along the direction with the higher power, 
$\mathbf{a}(\theta)^H R_x^{-1} \mathbf{a}(\theta)$ becomes small, so 
$\frac{1}{\mathbf{a}(\theta)^H R_x^{-1} \mathbf{a}(\theta)}$ becomes large. 
That is why there will be a peak. In the other case, the power will be small 
because the denominator is going to be large.


\subsubsection{Analysis of the main functions used in MVDR:}
\begin{itemize}
    \item The function "a\_lin" is the same as used in the previous section.  

    \item The function "MVDR" takes the same arguments as the "matchedbeamforming" function of the previous section, and computes and returns the same. 
\end{itemize}


\subsubsection{Testing}
Testing: how do we test, what are the results? (plots!)

Four configurations were used to acquire measurements to test the MVDR and fixed beamformer with. The configurations included an array of six microphones and one or two speakers.  The first three of the configurations included a single speaker that was placed at three different angles, one was placed 8 meters in front of the microphones (at an angle of zero degrees). In the second configuration the speaker was placed at an angle of 30 degrees and it was placed at an angle of 60 degrees in the third configuration. The fourth configuration contained two sources placed at 7 and -7 degrees. During the tests, the speakers were playing white noise, this was done to be able to select frequency slices while processing the data of the measurement to compare the results at different frequencies.



\paragraph{One source} The graph obtained from the measurements of the setup where the speaker was placed at $0^\circ$ can be seen below. The peak seen in the plot shows that the angle of the signal's source was placed at an angle of $-1^\circ$. This is a sufficiently accurate result as this is within the margin of error when placing the speaker for the measurement.

\begin{figure} [H]
    \centering                \includegraphics[width=0.8\textwidth]{docs/assignments/Midterm report/figures/module_3/MVDR/MVDR 1 source/MVDR_0.png}
         \caption{single source at 0 degrees}
         \label{fig:MVDR_0_degrees}
     \end{figure}


The plot obtained after changing the angle of the source to $30^\circ$ [\ref{fig:MVDR_30_degrees}] peaks at $\theta=30^\circ$ source accurately as well.

The graph acquired after measuring the response of the microphones with the source placed at $\theta=60^\circ$ [\ref{fig:MVDR_60_degrees}] shows the peak at about $60^\circ$.

\paragraph{Two sources} The fourth measurement configuration included two speakers placed in front of the microphone array. One of them was placed at an angle of $7^\circ$, the other one was placed at $-7^\circ$. As explained earlier in this module [\ref{aliasing}], when $\Delta$ surpasses $\frac{1}{2}$, spacial aliasing occurs. Because the frequency selected for this measurement was 3000Hz in stead of the previously used 1687 Hz, $\Delta$ surpassed $\frac{1}{2}$ (as $\Delta=\frac{d}{\lambda}$ and $\lambda=\frac{v}{f}$). The aliasing is visible on the ends of the graph obtained during this measurement [\ref{fig:MVDR_2_sources}].
\newpage

\subsection{Conclusion}


As the MVDR was explicitly designed to minimize the power response from angles that do not contain the source of the signal, it is reasonable to expect that its output is lower than the output of the other beamformer (except for the angles that do not contain the source which are 0, 30, and 60 respectively). This is confirmed by the plots where, especially in the first two configurations, the ripples of the beamformer[\ref{fig:beamformer_0_degrees} and \ref{fig:beamformer_30_degrees}] seem significantly larger than the ripples seen in the power response of the MVDR beamformer[\ref{fig:MVDR_0_degrees} and \ref{fig:MVDR_30_degrees}]. The rest of the single source measurements can be seen in \ref{fig:beamformer_60_degrees} and \ref{fig:MVDR_60_degrees}. \\
The same is observed when analyzing the results for the double source setup [\ref{fig:beamformer_2_source_3000Hz} and \ref{fig:MVDR_2_sources}].

Conclusion: summarize results, claims on expected accuracy



\section{Module 4}

Specifications: what is given, notation
Analysis: which problems have to be overcome
Design: 
The resulting design: implementation, main variables used
Testing: how do we test, what are the results? (plots!)
Conclusion: summarize results, claims on expected accuracy

\subsection{Specifications}

\subsection{Analysis}
\paragraph{SVD and Signal Subspace analysis}
As also explained by \cite{IP3Man}, when there are many sources, the signal \textbf{X} can be modeled as :
\begin{equation}
\mathbf{X} = \mathbf{A}\mathbf{S} : M \times N, \quad
\mathbf{A} = [\mathbf{a}_0 \ \mathbf{a}_1 \ \cdots \ \mathbf{a}_Q] : M \times Q, \quad
\mathbf{S} = \begin{bmatrix} 
\mathbf{s}_0 \\ 
\mathbf{s}_1 \\ 
\vdots \\ 
\mathbf{s}_Q 
\end{bmatrix} : Q \times N
\end{equation}

Where M is number of microphones, Q the number of sources and N the number of samples.
Because the columns of \textbf{X} are linear combinations of the vectors inside \textbf{A}, the rank of \textbf{X} will be the number of sources Q.

According to the properties of the Singular Value decomposition (SVD), the columns of the matrix \textbf{U} will be orthonormal.Also, as shown in \cite{IP3Man} and in \ref{fig:SVD_interpretation}, the dominant singular vectors of the matrix \textbf{U} span the same subspace as the matrix \textbf{A}, but orthogonally. Additionally, the singular vectors that are really small (zero) span the subspace of the noise \textbf{$U_n$}.
$$
\mathbf{X} = \mathbf{A}\mathbf{S}, \qquad \mathbf{X} = \mathbf{U}\Sigma\mathbf{V}^H.
$$

\paragraph{MUSIC Algorithm}

Thus, the matrix \textbf{U} comprises of the matrix \textbf{$U_s$} which contains the dominant singular vectors that span the signal and are equal to the number of sources Q, and the matrix \textbf{$U_n$} whose vectors span the noise subspace.

As a result, because \textbf{A} and \textbf{$U_s$} are the same thing and \textbf{$U_n$} is orthogonal to \textbf{$U_s$}, if a steering vector $\mathbf{a}(\theta)$ points to an angle where there is a source (so this steering vector belongs to \textbf{A}), \textbf{$U_n$} will be orthogonal to it.


The formulae below express these properties to help in finding the direction of arrival.
$$
P_{\text{music}}(\theta) = \frac{1}{\mathbf{a}^H(\theta)\mathbf{U}_n\mathbf{U}_n^H\mathbf{a}(\theta)}, \qquad \theta \in [-\pi/2, \pi/2].
$$

$$
\mathbf{a}(\theta_0) \perp \mathbf{U}_n \quad \Rightarrow \quad \mathbf{U}_n^H\mathbf{a}(\theta_0) = \mathbf{0} \quad \Rightarrow \quad \mathbf{a}^H(\theta_0)\mathbf{U}_n\mathbf{U}_n^H\mathbf{a}(\theta_0) = 0.
$$
So there will be peaks in the spectrum, when the angle is one of the source angles, because the denominator will be zero.

Thus the only thing to find from \textbf{X} is the noise subspace \textbf{$U_n$}. This can be found either from the SVD of from the eigen decomposition of the auto covariance Rx. As proven in \cite{IP3Man}, the eigenvectors of Rx are the same as the left singular vectors of \textbf{X}.


\subsection{Design}
The main functions used are:
\begin{itemize}
    \item The function "a\_lin", which is the same as before.

    \item The function "music" takes the microphone data matrix \(X\), the number of sources \(Q\), and the array parameters \(M\), \(d\), \(v\), and \(f_0\). From the data, the sample covariance matrix
    \[
    R_x = \frac{1}{N} X X^H
    \]
    is computed and an eigenvalue decomposition of \(R_x\) is performed. The \(M-Q\) eigenvectors corresponding to the smallest eigenvalues are used to form the noise subspace \(U_n\). For a range of candidate angles \(\theta \in [-90^\circ,90^\circ]\), the MUSIC pseudospectrum
    \[
    P_{\text{MUSIC}}(\theta) = \frac{1}{a^H(\theta) U_n U_n^H a(\theta)}
    \]
    is evaluated. The resulting spectrum is returned, where peaks indicate the estimated directions of arrival.

\end{itemize}


\subsection{Testing}

\paragraph{One source}
For one source the results of the MUSIC were satisfactory.

\begin{figure} [H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/assignments/Midterm report/figures/module_4/MUSIC/MUSIC Single Source/Music_0_degrees.png}
    \caption{Beamformer at +7$^\circ$, -7$^\circ$, higher resolution.}  
    \label{fig:music_1_source_0_degrees}
\end{figure}
It is apparent that the lobe is really sharp, exactly because of the way the MUSIC works. There is a small offset of 0.5$^\circ$, but this can be due to set up errors. 

The plot for $30^\circ$ can be seen here [\ref{fig:music_1_source_30_degrees}]. There is also a really small offset of $30.332^\circ$, but offsets that small can be negligible.

The plot for $60^\circ$ can be seen here [\ref{fig:music_1_source_60_degrees}]. A larger offset of $4.82^\circ$ is visible. Also aliasing has started to appear. Aliasing is logical because with the chosen central frequency, we are on the verge of aliasing as explained in \ref{central_frequency_selection}. Also it is clear that aliasing is just at the start, because it happens only on the sides of the graph.


\paragraph{Two Sources}
For 2 sources the situation for MUSIC is similar to \ref{beamformer_resolution_2_sources_problem} and can be seen in \ref{fig:music_2_source_1687}. So there needs to be a higher central frequency for the peaks to separate.

This is seen with 3185.5 \textbf{Hz}. 
\begin{figure} [H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/assignments/Midterm report/figures/module_4/MUSIC/MUSIC 2 sources/Bin freq = 3185.5.png}
    \caption{Beamformer at +7$^\circ$, -7$^\circ$, higher resolution.}  
    \label{fig:music_1_source_0_degrees}
\end{figure}

The peak on the right comes due to aliasing and is not the actual signal. This is because, in lower frequencies [\ref{fig:music_2_source_1687}], the most "power" is concentrated in the centre. Thus the area to search while increasing the frequency is the area that the big lobe takes at [\ref{fig:music_2_source_1687}].
It can be seen that with high enough frequency, MUSIC lobes are really sharp and the accuracy is impecable with less than a degree of error per side.
